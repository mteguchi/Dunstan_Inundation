---
title: "R Notebook"
output: html_notebook
---

Request from Andy Dunstan on the following problem.

Email from AD on Feb 24, 2020:

Here is another query for your advice if you have the time and inclination – much appreciated if you do. It’s another one of those limited data situations where there is a good time series but limited number of logger locations available. Am wondering if this may suit a statistical model similar to the limited sample data points available for hatchling counts?

We wish to determine a best fit maximum inundation curve (berm to back swale across the beach) for months of Dec, Jan, Feb and March if possible. At the moment the model used is the best fit regression curve from peak inundation data at each logger location. The distance of logger locations from the berm does change with island shifts and changes to the berm over season and years and has been measured with dGPS surveys. This provides a range of slightly different logger-berm distance points to use and hopefully increases the confidence levels in a calculated curve. However …. am hoping there may be a better method to fit the curve and provide confidence limits or if this is indeed the best model then a good suggestion for statistical justification of this curve?

This curve would then be correlated with nest locations (distance from berm) and elevations (calculated from the curve where x-axis is distance from berm) to infer inundation from the curve. (eg. Above top of nest, marginal to level of middle of nest or no inundation as three categories). Data suggests some variation in inundation curve profiles around the island at the four different transects however not too radical. There is a definite outlier to this in the NW (mooring) beach area when Westerly winds have lowered the berm and peak tide levels are above this berm height resulting in inundation across the entire beach width – can be treated separately.

End of AD's email.

Additionally, the original request is from Owen Coffee:

The intention in this workbook is to define a predicted monthly max inundation curve for the whole of island based on the inundation data from the loggers at the 4 transects around the beach. At present the produced predicted curves are the log. Regression of the max inundation points for each month ± a 95% conf. interval. The relationships between the curves however are variable which look to be related to sample size and variation between sectors. Having defined these predicted inundation curves the marked nests are then plotted against these predictions and any nest with the 95% conf. interval or below the inundation height is predicted to have been inundated. The intention is then to review the hatching success and development phases of failed eggs for these nests identified as inundated to determine whether the predicted inundation events correlate with the phases of failed eggs.

Any comments/suggestions on improvements in determining the best regression model to fit the different months or potentially an improved method to define a predicted monthly max inundation curve which allows smoothing of the produced curve, and whether it possible to automate the identification of inundated nests (as this process is currently manual) would be greatly appreciated.

End of OC's email

First, as usual, I look at the data in different ways.

Load essential libraries first.

```{r}
rm(list=ls())
library(ggplot2)
library(tidyverse)
library(readr)
library(lubridate)
```

And load the data.

```{r}

inund.cols <- cols(
   Transect = col_double(),
   Post = col_double(),
   InundHeight_AbvLAT = col_double(),
   Dist_Berm = col_double(),
   Berm_elevation = col_double(),
   BermSurvey_Month = col_character(),
   Inund_Month = col_character(),
   Date = col_date(format = "%d/%m/%Y"),
   Time = col_time(format = "%H:%M:%S %p"))

inund <- read_csv("data/inundationRaine.csv", col_types = inund.cols)
#View(inund)

nestmap.cols <- cols(
     Nest_ID = col_double(),
     Nest_Easting = col_double(),
     Nest_Northing = col_double(),
     Nest_Elevation = col_double(),
     BermDist_m = col_double())

Nest_18Map <- read_csv("data/2018Nest_mapping.csv", col_types = nestmap.cols)
#View(Nest_18Map)

nest18.cols <- cols(
   Nest_ID = col_double(),
   Nest_Easting = col_double(),
   Nest_Northing = col_double(),
   Nest_Elevation = col_double(),
   BermDist_m = col_double(),
   inundated = col_character())
Nest_18INUND <- read_csv("data/2018Nest_INUND.csv", col_types = nest18.cols)
```


Make some plots:

```{r}
ggplot() + 
  geom_point(data = inund,
             aes(x = Dist_Berm,
                 y = InundHeight_AbvLAT,
                 color = Inund_Month,
                 shape = as.factor(Transect)))
```

And in the log scale:

```{r}
ggplot() + 
  geom_point(data = inund,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT),
                 color = Inund_Month,
                 shape = as.factor(Transect)))
```

Even at the log scale, some don't look linear (e.g., transect 5 (crosses)). Take a look at them more closely.  First, split them into two years.

```{r}
inund %>% mutate(Year = ifelse(Date < as.Date("2018-05-01"), 2018, 2019)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Transect_f = as.factor(Transect)) -> inund

inund.2018 <- filter(inund, Year == 2018)
inund.2019 <- filter(inund, Year == 2019)

```

Then plot one year at a time.

```{r}
ggplot() + 
  geom_point(data = inund.2018,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT),
                 color = as.factor(Month),
                 shape = Transect_f))
```


```{r}
ggplot() + 
  geom_point(data = inund.2019,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT),
                 color = as.factor(Month),
                 shape = as.factor(Transect)))
```

I think non-linear models (or polynomial) are probably better than linear models...  

```{r}
ggplot() + 
  geom_point(data = inund.2018,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT),
                 color = as.factor(Month))) +
  facet_wrap(~ Transect_f, nrow = 2)  
  #geom_smooth(method = "loess", size = 1.5)    # this doesn't work - too small sample size

# ggsave(filename = "figures/InundHeight_2018.png",
#        device = "png", dpi = 600)
```

```{r}
ggplot() + 
  geom_point(data = inund.2019,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT),
                 color = as.factor(Month))) +
  #facet_grid(. ~ Transect_f) +
  facet_wrap(~ Transect_f, nrow = 2)

# ggsave(filename = "figures/InundHeight_2019.png",
#        device = "png", dpi = 600)
```

When I add Berm_elevation, variability decreases... maybe on to something? 

```{r}
ggplot() + 
  geom_point(data = inund.2018,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT + Berm_elevation),
                 color = as.factor(Month))) +
  #facet_grid(. ~ Transect_f) +
  facet_wrap(~ Transect_f, nrow = 2)

# ggsave(filename = "figures/InundHeight_Berm_2018.png",
#        device = "png", dpi = 600)
```



```{r}
ggplot() + 
  geom_point(data = inund.2019,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT + Berm_elevation),
                 color = as.factor(Month))) +
  #facet_grid(. ~ Transect_f) +
  facet_wrap(~ Transect_f, nrow = 2)

# ggsave(filename = "figures/InundHeight_Berm_2019.png",
#        device = "png", dpi = 600)
```

We should be able to have a model that is fitted to all transects but each transect having its slope and intercept (random slope and intercept). 

First, transform some variables:

```{r}
inund %>% mutate(Year2 = ifelse(Year == 2018, 1, 2),
                 Month2 = ifelse(Month > 9, Month - 9, Month + 3),
                 Transect2 = Transect - 1,
                 log_inund = log(InundHeight_AbvLAT)) -> inund


```

The small sample size makes it difficult to fit complicated models. Separating by months will not work with just a couple data points in some cases. So, I'm going to include all months. Then, extract just Dec - March. In the next two figures, data from Oct, Nov, and April are in orange. Will eliminating those will affect regression equations? 

```{r}
inund.2018 %>% filter(Month > 11 | Month < 4) -> inund.2018.1
inund.2018 %>% filter(Month < 12 & Month > 3) -> inund.2018.2

ggplot() + 
  geom_point(data = inund.2018.1,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT))) +
  geom_point(data = inund.2018.2,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT)),
             color = "orange") +
  facet_wrap(~ Transect_f, nrow = 2)  
  #geom_smooth(method = "loess", size = 1.5)    # this doesn't work - too small sample size

# ggsave(filename = "figures/InundHeight_2018.png",
#        device = "png", dpi = 600)
```

For 2018, including those three months will affect transects 2 and 3 a little (lower intercepts and slope) but probably not much for transects 4 and 5. 

```{r}
inund.2019 %>% filter(Month > 11 | Month < 4) -> inund.2019.1
inund.2019 %>% filter(Month < 12 & Month > 3) -> inund.2019.2

ggplot() + 
  geom_point(data = inund.2019.1,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT))) +
  geom_point(data = inund.2019.2,
             aes(x = Dist_Berm,
                 y = log(InundHeight_AbvLAT)),
             color = "orange") +
  facet_wrap(~ Transect_f, nrow = 2)  
  #geom_smooth(method = "loess", size = 1.5)    # this doesn't work - too small sample size

# ggsave(filename = "figures/InundHeight_2018.png",
#        device = "png", dpi = 600)
```

For 2019, including those three months will provide regression for transect 2. No data were available for transect 2 in Dec through March. Just looking at these plots, I don't think slopes or intercepts will change significantly just by having these data points. Leaving them in will give us a few more data points, which are useful when the entire dataset is so small. 


```{r}
library(jagsUI)
library(bayesplot)

jags.data <- list(y = inund$log_inund,
                  transect = inund$Transect2,
                  Dist_Berm = inund$Dist_Berm,
                  year = inund$Year2,
                  N = nrow(inund))

MCMC.params <- list(n.samples = 10000,
                    n.burnin = 5000,
                    n.thin = 5,
                    n.chains = 5)

parameters <- c("b0", "b1", "b2", "sigma.y", "deviance")

MCMC.params$model.file = "models/Model_linear_all.txt"

#if (!file.exists("RData/linear_all.rds")){
  jm <- jags(data = jags.data,
             #inits = inits,
             parameters.to.save= parameters,
             model.file = MCMC.params$model.file,
             n.chains = MCMC.params$n.chains,
             n.burnin = MCMC.params$n.burnin,
             n.thin = MCMC.params$n.thin,
             n.iter = MCMC.params$n.samples,
             DIC = T, 
             parallel=T)

#   saveRDS(jm, file = "RData/linear_all.rds")  
# } else {
#   jm <- readRDS("RData/linear_all.rds")
# }

summary(jm)  

# check some posteriors
mcmc_trace(jm$samples, pars = c("b0[1]", "b0[2]", "b0[3]", "b0[4]"))
mcmc_trace(jm$samples, pars = c("b1[1]", "b1[2]", "b1[3]", "b1[4]"))
mcmc_trace(jm$samples, pars = c("b2[1]", "b2[2]"))

# they all look good.

```

Do predictions.

```{r}
extract.samples <- function(varname, zm){
  dev <- unlist(lapply(zm, FUN = function(x) x[, varname]))
  return(dev)
}

b0 <- b1 <- vector(mode = "list", length = 4)
for (k in 1:4){
  b0[[k]] <- extract.samples(paste0("b0[", k, "]"), jm$samples)
  b1[[k]] <- extract.samples(paste0("b1[", k, "]"), jm$samples)
}

b2 <- vector(mode = "list", length = 2)
for (k in 1:2){
  b2[[k]] <- extract.samples(paste0("b2[", k, "]"), jm$samples)
}

Dvec <- seq(0, 80, by = 1)  # min and max of Distance from Berm
Yvec <- c(1, 2)

pred.Y <- vector(mode = "list", length = 8)
k <- y <- c <- 1
for (k in 1:4){      # transects
  for (y in 1:2){    # years
    pred.Y[[c]] <- b0[[k]] + b1[[k]] %*% t(Dvec) + b2[[y]] * Yvec[y]
    #pred.Y.med[[c]] <- jm$q50$b0[k] + jm$q50$b1[k] %*% t(Dvec) + jm$q50$b2[y] * Yvec[y]
    #pred.Y.high[[c]] <- jm$q97.5$b0[k] + jm$q97.5$b1[k] %*% t(Dvec) + jm$q97.5$b2[y] * Yvec[y]
    c <- c + 1
  }
  
}

# get quantiles

qtiles <- lapply(pred.Y, 
                 FUN = function(x) apply(x, MARGIN = 2, 
                                         FUN = quantile, c(0.025, 0.5, 0.975)))

qtiles.mat <- data.frame(do.call(rbind, lapply(qtiles, FUN = t)))

year.vec <- rep(rep(c(1,2), each = length(Dvec)), 4)
transect.vec <- rep(c(1,2, 3, 4), each = length(Dvec) * length(Yvec))

pred.df <- data.frame(Transect_f = as.factor(transect.vec + 1), 
                       Year = year.vec + 2017, 
                       Dist_Berm = rep(Dvec, 4 * length(Yvec)),
                       pred_low = qtiles.mat$X2.5., 
                       pred_med = qtiles.mat$X50., 
                       pred_high = qtiles.mat$X97.5.)

pred.df %>% filter(Year == 2018) -> pred.df.2018
ggplot() + 
  geom_point(data = inund.2018,
             aes(x = Dist_Berm,
                 y = InundHeight_AbvLAT)) +
  geom_path(data = pred.df.2018,
            aes(x = Dist_Berm,
                 y = exp(pred_med))) +
  geom_ribbon(data = pred.df,
              aes(x = Dist_Berm,
                  ymin = exp(pred_low),
                  ymax = exp(pred_high)),
              alpha = 0.5) +
  facet_wrap(~ Transect_f, nrow = 2)  

```

Not sure why bands are jagged... they should be smooth...

```{r}
pred.df %>% filter(Year == 2019) -> pred.df.2019
ggplot() + 
  geom_point(data = inund.2019,
             aes(x = Dist_Berm,
                 y = InundHeight_AbvLAT)) +
  geom_path(data = pred.df.2019,
            aes(x = Dist_Berm,
                 y = exp(pred_med))) +
  geom_ribbon(data = pred.df,
              aes(x = Dist_Berm,
                  ymin = exp(pred_low),
                  ymax = exp(pred_high)),
              alpha = 0.5) +
  facet_wrap(~ Transect_f, nrow = 2)  

```

