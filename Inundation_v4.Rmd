---
title: "R Notebook"
output: html_notebook
---

Request from Andy Dunstan on the following problem.

Email from AD on Feb 24, 2020:

Here is another query for your advice if you have the time and inclination – much appreciated if you do. It’s another one of those limited data situations where there is a good time series but limited number of logger locations available. Am wondering if this may suit a statistical model similar to the limited sample data points available for hatchling counts?

We wish to determine a best fit maximum inundation curve (berm to back swale across the beach) for months of Dec, Jan, Feb and March if possible. At the moment the model used is the best fit regression curve from peak inundation data at each logger location. The distance of logger locations from the berm does change with island shifts and changes to the berm over season and years and has been measured with dGPS surveys. This provides a range of slightly different logger-berm distance points to use and hopefully increases the confidence levels in a calculated curve. However …. am hoping there may be a better method to fit the curve and provide confidence limits or if this is indeed the best model then a good suggestion for statistical justification of this curve?

This curve would then be correlated with nest locations (distance from berm) and elevations (calculated from the curve where x-axis is distance from berm) to infer inundation from the curve. (eg. Above top of nest, marginal to level of middle of nest or no inundation as three categories). Data suggests some variation in inundation curve profiles around the island at the four different transects however not too radical. There is a definite outlier to this in the NW (mooring) beach area when Westerly winds have lowered the berm and peak tide levels are above this berm height resulting in inundation across the entire beach width – can be treated separately.

End of AD's email.

Additionally, the original request is from Owen Coffee:

The intention in this workbook is to define a predicted monthly max inundation curve for the whole of island based on the inundation data from the loggers at the 4 transects around the beach. At present the produced predicted curves are the log. Regression of the max inundation points for each month ± a 95% conf. interval. The relationships between the curves however are variable which look to be related to sample size and variation between sectors. Having defined these predicted inundation curves the marked nests are then plotted against these predictions and any nest with the 95% conf. interval or below the inundation height is predicted to have been inundated. The intention is then to review the hatching success and development phases of failed eggs for these nests identified as inundated to determine whether the predicted inundation events correlate with the phases of failed eggs.

Any comments/suggestions on improvements in determining the best regression model to fit the different months or potentially an improved method to define a predicted monthly max inundation curve which allows smoothing of the produced curve, and whether it possible to automate the identification of inundated nests (as this process is currently manual) would be greatly appreciated.

End of OC's email

In this version (v.4), not just the high tide but "above median" tides were included in this analysis. OC's email on 2020-08-05:

"The original model was based on the highest recorded for the month, and operated on a flawed assumption that only an extreme high would have been sufficient to inundate nests. The consequent produced curve was estimating only that one events inundation height along the beach profile for the whole island for those sections on the beach profile where we didn't have an inundation logger. But it wasn't able to estimate the height on days we didn't have the data which is why the new working file has all highs above the median for each month."

A new data file was sent and I'm using the new input file for this analysis.

After a conversation with OC on 2020-08-06, I realized my understanding of his intentions was incorrect. He wanted to have two sets of model fit for each month, where each set is based on 5-6 days of water level data. This may require a little bit of tweaking in my models. Let's take a look at the data first and see what they look like.

Load essential libraries first.

```{r}
rm(list=ls())
library(ggplot2)
library(tidyverse)
library(readr)
library(lubridate)
library(jagsUI)
library(bayesplot)

source("Dunstan_Inundation_fcns.R")
```

And load the data.

Remove transect 2 as per discussion on 2020-03-16 (US Westcoast time).
First, transform some variables:

```{r}

inund.cols <- cols(
   Transect = col_double(),
   Post = col_double(),
   Date = col_date(format = "%d/%m/%Y"),
   Time = col_time(format = "%H:%M:%S %p"),
   Aspect = col_character(),
   Survey_Date = col_character(),
   Inundation_Month = col_character(),
   Dist_from_Berm = col_double(),
   Beach_Elevation = col_double(),
   Inund_Height = col_double())

inund <- read_csv("data/inundationRaine_abovemedian.csv", col_types = inund.cols)

inund %>% filter(Transect != 2) %>%
  mutate(Transect_f = as.factor(Transect),
         Year = ifelse(Date < as.Date("2018-05-01"), 2018, 2019),
         Year2 = ifelse(Year == 2018, 1, 2),
         Transect2 = Transect - 1,
         log_inund = log(Inund_Height)) %>% 
  arrange(Date)-> inund

# Email from OC on 2020-08-10
# There was an anomalous additional peak that was observed in post 5.1 which resulted in the 9-13/12 dataset. Given it was only observed once and is recorded in a section of the beach where it is unlikely to have affected any nests (2017 reprofiled) I don't mind if it is excluded and only 2-7 and 15-20 are used for December. for the 31 Dec -05 Jan dataset, happy to just use the Jan readings (i.e. 1-5 Jan) and remove the 31st unless it is particularly high.

# So, use only 2017-12-02 to 2017-12-07 and 2017-12-15 to 2017-12-20, then remove 2017-12-31.
inund %>% filter((Date < as.Date("2017-12-08") | Date > as.Date("2017-12-14"))) %>%  # remove between 12-08 and 12-04 
  filter(Date != as.Date("2017-12-31")) -> inund

# define datasets by looking at difference in consecutive dates.
dt <- as.numeric(inund$Date[2:length(inund$Date)] - inund$Date[1:(length(inund$Date)-1)])
inund$dt <- c(0, dt)
inund$dataset.ID <- cumsum(ifelse(inund$dt < 2, 0, 1)) + 1

# Datasets are defined by how data are combined among close dates. So, sometimes, they go over two
# months. These have to be assigned to the same month as the first data point of the particular
# dataset.
inund %>% group_by(dataset.ID) %>%
  mutate(Month = month(nth(Date, 3)),
         Month2 = ifelse(Month > 9, Month - 9, Month + 3),
         Month_f = factor(Month, levels = c("10", "11", "12", "1", "2", "3", "4"))) -> inund

# create dataset vs. month/year look up table
inund %>% 
  select(dataset.ID, Month, Month2, Year2) %>% 
  group_by(dataset.ID) %>% 
  summarise(Month = first(Month), 
            Month2 = first(Month2),
            Year2 = first(Year2)) -> dataset_def 
```

Explore data with making some plots. In the previous data exploration, I found that variability among transects is quite small. So, I can pool them to model. 


```{r}
#tmp <- filter(inund, Month == 11)
ggplot() + 
  geom_point(data = inund,
             aes(x = Dist_from_Berm,
                 y = Inund_Height,
                 color = as.factor(dataset.ID))) +
  facet_grid(Year ~ Month_f) +
  theme(legend.position = "none")
```
START HERE ON 2020-08-07

I decided to fit a few polynomial functions and the exponential decay function with dataset and year specific intercepts but month-specific coefficients for the distance from the berm. This allows us to shift the intercept between datasets, within a month, and between the years whereas treat data for two years together to compute how the inundation height decreases as the distance increases. 

To minimize the possibility of exploding multiplications, I transformed the distance from the berm in the following way:

scaled_Dist_from_Berm = (Dist_from_Berm - min.Dist)/sd.Dist,

where min.Dist is the minimum distance in Dist_from_Berm (two years combined) and sd.Dist is
the standard deviation of all Dist_from_Berm values (two years combined). 

The response variable (Inund_Height) is log transformed. 

I eliminated a linear model because the data don't look linear. 

```{r}
# I re-scale the distance from the berm, in order to make the
# exponentiation not blow up. 
# In this version, models need to be fit to each dataset.ID, not to month/year combination

jags.data <- list(dataset.ID = inund$dataset.ID,
                  y = inund$log_inund,
                  month = inund$Month2,
                  Dist_Berm = inund$Dist_from_Berm,
                  year = inund$Year2,
                  N = nrow(inund),
                  n.dataset = length(unique(inund$dataset.ID)),
                  n.years = length(unique(inund$Year2)),
                  n.months = length(unique(inund$Month2)))

MCMC.params <- list(n.samples = 10000,
                    n.burnin = 5000,
                    n.thin = 5,
                    n.chains = 5)

```

Second order polynomial with dataset-specific intercept and coefficients for the distance plus year effect:

mu.y[i] <- b0[dat.ID[i]] + b1[dat.ID[i]] * Dist_from_Berm[i] + b12[dat.ID[i]] * Dist_from_Berm[i] * Dist_from_Berm[i] + b2 * year[i]

y[i] ~ dnorm(mu.y[i], tau.y)  

```{r}
MCMC.params$model.file = "models/Model_Poly2_dataset_year.txt"

parameters <- c("b0", "b1", "b2", "b12", "sigma.y",
                 "fit", "fit.new", "deviance")

if (!file.exists("RData/Poly2_dataset_year_aboveMed.rds")){
  jm.2 <- jags(data = jags.data,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
  
  saveRDS(jm.2, file = "RData/Poly2_dataset_year_aboveMed.rds")  
} else {
  jm.2 <- readRDS("RData/Poly2_dataset_year_aboveMed.rds")
}

summary(jm.2)  
```


And density plots:
```{r}
# check some posteriors
mcmc_dens(jm.2$samples, pars = c("b0[1]", "b0[2]", "b0[3]", "b0[4]", 
                                  "b0[5]", "b0[6]", "b0[7]", "b0[8]", "b0[9]"))

```

First-order coefficients:

```{r}
mcmc_dens(jm.2$samples, pars = c("b1[1]", "b1[2]", "b1[3]", "b1[4]",
                                "b1[5]", "b1[6]", "b1[7]", "b1[8]", "b1[9]"))

```

A problem in the 3rd and 6th dataset.

Second order coefficients

```{r}

mcmc_dens(jm.2$samples, pars = c("b12[1]", "b12[2]", "b12[3]", "b12[4]",
                                "b12[5]", "b12[6]", "b12[7]", "b12[8]", "b12[9]"))
```

That's a problem... 

And the year effect:
```{r}
mcmc_dens(jm.2$samples, pars = c("b2"))
```

Extract the posterior samples and create plots
```{r}
pred.list.jm.2 <- extract.prediction.poly2.dataset(zm = jm.2, 
                                                   jags.data = jags.data, 
                                                   dataset_def = dataset_def)

# need to transform dataset ID to month:
pred.list.jm.2$pred.df %>% left_join(dataset_def, by = "dataset.ID") -> pred.df.jm.2

pred.df.jm.2 %>% 
  mutate(Year = Year2 + 2017,
         Month_f = factor(Month, 
                          levels = c("10", "11", "12", "1", "2", "3", "4"))) -> pred.df.jm.2

#pred.df.jm.2 %>% filter(Year == 2018) -> pred.df.jm.2.2018
ggplot() + 
  geom_point(data = inund,
             aes(x = Dist_from_Berm,
                 y = log(Inund_Height),
                 group = dataset.ID)) +
  geom_path(data = pred.df.jm.2 %>% group_by(dataset.ID),
            aes(x = Dist_Berm,
                y = (pred_med),
                group = dataset.ID)) +
  # geom_ribbon(data = pred.df.jm.2,
  #             aes(x = Dist_Berm,
  #                 ymin = (pred_low),
  #                 ymax = (pred_high)),
  #             alpha = 0.5) +
  facet_grid(Year ~ Month_f)  

```

The problem is that one datset (dataset ID = 3) contained only one distance from the berm... so, with dataset specific intercepts and coefficients, it can't estimate the coefficients for the dataset. 

We can change that with having dataset specific intercepts but month specific coefficients for other parameters.

```{r}

MCMC.params$model.file = "models/Model_Poly2_dataset_month_year.txt"

parameters <- c("b0", "b1", "b2", "b12", "sigma.y",
                 "fit", "fit.new", "deviance")

if (!file.exists("RData/Poly2_dataset_month_year_aboveMed.rds")){
  jm.2.1 <- jags(data = jags.data,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
  
  saveRDS(jm.2.1, file = "RData/Poly2_dataset_month_year_aboveMed.rds")  
} else {
  jm.2.1 <- readRDS("RData/Poly2_dataset_month_year_aboveMed.rds")
}

summary(jm.2.1)  
```

And density plots:
```{r}
# check some posteriors
mcmc_dens(jm.2.1$samples, pars = c("b0[1]", "b0[2]", "b0[3]", "b0[4]", 
                                  "b0[5]", "b0[6]", "b0[7]", "b0[8]", "b0[9]"))

```

First-order coefficients:

```{r}
mcmc_dens(jm.2.1$samples, pars = c("b1[1]", "b1[2]", "b1[3]", "b1[4]",
                                "b1[5]", "b1[6]", "b1[7]"))

```

Second order coefficients

```{r}

mcmc_dens(jm.2.1$samples, pars = c("b12[1]", "b12[2]", "b12[3]", "b12[4]",
                                "b12[5]", "b12[6]", "b12[7]"))
```


And the year effect:
```{r}
mcmc_dens(jm.2.1$samples, pars = c("b2"))
```

This is mostly zero... 

Extract the posterior samples and create plots
```{r}
pred.list.jm.2.1 <- extract.prediction.poly2.dataset.month(zm = jm.2.1, 
                                                           jags.data = jags.data, 
                                                           dataset_def = dataset_def)

sigma.y <- pred.list.jm.2.1$sigma.y

# need to transform dataset ID to month:
pred.list.jm.2.1$pred.df %>% left_join(dataset_def, by = "dataset.ID") -> pred.df.jm.2.1

pred.df.jm.2.1 %>% 
  mutate(Year = Year2 + 2017,
         Month_f = factor(Month, 
                          levels = c("10", "11", "12", "1", "2", "3", "4"))) -> pred.df.jm.2.1

#pred.df.jm.2 %>% filter(Year == 2018) -> pred.df.jm.2.2018
ggplot() + 
  geom_point(data = inund,
             aes(x = (Dist_from_Berm),
                 y = log(Inund_Height))) +
  geom_path(data = pred.df.jm.2.1,
            aes(x = Dist_Berm,
                y = (pred_med),
                group = dataset.ID)) +
  geom_ribbon(data = pred.df.jm.2.1,
              aes(x = Dist_Berm,
                  ymin = (pred_low),
                  ymax = (pred_high),
                  group = dataset.ID),
              alpha = 0.5) +
  geom_ribbon(data = pred.df.jm.2.1,
              aes(ymin = pred_med - median(sigma.y) * 2,
                  ymax = pred_med + median(sigma.y) * 2,
                  x = Dist_Berm,
                  group = dataset.ID),
              fill = "orange",
              alpha = 0.4) +
  facet_grid(Year ~ Month_f)  +
  theme(legend.position = "none")

#ggsave(filename = "figures/poly2_pred_dataset_month.png", dpi = 600, device = "png")
```

